{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harikrishnakancharla/Gen-AI/blob/main/002241638_ChatGPT_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=\"#7C7965\"># Applied Generative AI</font>\n",
        "### Student: Hari Krishna Kancharla\n",
        "\n",
        "Welcome to the **Applied Generative AI** course. In this course, we will explore the foundations and applications of Generative AI using tools like OpenAI's API. By the end of this session, you will:\n",
        "\n",
        "- üü¢ **Understand** how to set up and connect to OpenAI's API.  \n",
        "- üåü **Learn** about the roles (System, Assistant, User) in prompt design.  \n",
        "- ‚ú® **Generate** text, images, and vector embeddings programmatically.  \n",
        "- üîß **Explore** fine-tuning to customize AI models for specific tasks.  \n",
        "\n",
        "<font color=\"#BFBB9B\">Let‚Äôs get started with setting up the OpenAI API!</font>\n"
      ],
      "metadata": {
        "id": "5Y0iCPL2SP3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "07hivz97R7fL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42771ef-f3aa-4746-fc1f-28e6fc13ffc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.11/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from openai==0.28) (3.11.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->openai==0.28) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->openai==0.28) (1.18.3)\n"
          ]
        }
      ],
      "source": [
        "# Install the OpenAI Python SDK\n",
        "# This library allows us to interact with OpenAI's API for text, images, and embeddings.\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # For setting environment variables, such as API keys\n",
        "import ipywidgets as widgets  # For creating interactive input widgets in Jupyter or Colab"
      ],
      "metadata": {
        "id": "L22tG3HECbUg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenAI library for interacting with the API\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "# Replace \"YOUR_API_KEY\" with your actual key from OpenAI\n",
        "openai.api_key = \"Your-API-Key\"\n",
        "\n",
        "try:\n",
        "    # Test the connection by sending a basic request to the GPT model\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Specify the model to use\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Connection test\"}]  # Provide a simple test message\n",
        "    )\n",
        "    # Print success message if the API call works\n",
        "    print(\"Connection successful!\")\n",
        "except Exception as e:\n",
        "    # Print the error message if the API call fails\n",
        "    print(\"Connection failed:\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s_rCaZogYWZ_",
        "outputId": "29dc1397-db0a-475c-8e05-44b039e58155"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection failed: Invalid API Key. Check if your API key is correct.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCcEhJJMTsaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110c272e-8baf-4231-84f0-eaa3617df40c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection failed: Incorrect API key provided: Your-API-Key. You can find your API key at https://platform.openai.com/account/api-keys.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3J3ZKzeBO9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Prompt Playground: Understanding Roles\n",
        "\n",
        "In OpenAI's API, you interact with the model using **roles**, which define the flow of the conversation:\n",
        "- ‚û°Ô∏è **System**: Sets the behavior and tone of the assistant (e.g., \"You are a cheerful assistant.\").\n",
        "- ‚û°Ô∏è **User**: Represents the input or question from the user (e.g., \"What is AI?\").\n",
        "- ‚û°Ô∏è **Assistant**: Automatically generated responses based on the system and user inputs.\n",
        "\n",
        "üí° **Why Roles Matter**:\n",
        "Roles help control the assistant's personality and the quality of responses. For example:\n",
        "- A system message like \"You are a strict teacher\" makes the assistant respond more formally.\n",
        "- A system message like \"You are a friendly chatbot\" leads to casual responses.\n",
        "\n",
        "Let‚Äôs see how these roles work in the next cell!\n",
        "\n",
        "###Example 1: Without Assistant Role"
      ],
      "metadata": {
        "id": "jPntdyswUWZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating roles in OpenAI's API without the assistant role\n",
        "\n",
        "# Define the conversation using only system and user roles\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor who explains problems step by step.\"},  # System role sets the behavior\n",
        "    {\"role\": \"user\", \"content\": \"Solve for x: 2x + 5 = 15\"}  # User question\n",
        "]\n",
        "\n",
        "# Send the conversation to the API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",  # Use the chosen model\n",
        "    messages=messages  # Pass the conversation\n",
        ")\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "id": "fpgFdlQBUW-r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "7ada2a08-0046-4249-a9fd-4df019e382f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AuthenticationError",
          "evalue": "Incorrect API key provided: Your-API-Key. You can find your API key at https://platform.openai.com/account/api-keys.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e796372111db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Send the conversation to the API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Use the chosen model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m  \u001b[0;31m# Pass the conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: Your-API-Key. You can find your API key at https://platform.openai.com/account/api-keys."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example 2: With Assistant Role"
      ],
      "metadata": {
        "id": "qoQ3W_fshgV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demonstrating roles in OpenAI's API with the assistant role\n",
        "\n",
        "# Define the conversation including a predefined assistant response\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor who explains problems step by step.\"},  # System role sets the behavior\n",
        "    {\"role\": \"user\", \"content\": \"Solve for x: 2x + 5 = 15\"},  # User question\n",
        "    {\"role\": \"assistant\", \"content\": \"To solve for x: \\n1. Subtract 5 from both sides: 2x = 10\\n2. Divide both sides by 2: x = 5\"}  # Predefined assistant response\n",
        "]\n",
        "\n",
        "# Send the conversation to the API\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",  # Use the chosen model\n",
        "    messages=messages  # Pass the conversation\n",
        ")\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "id": "7IySAOCAUoWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üßÆ Hands-On"
      ],
      "metadata": {
        "id": "aJBWZ_RSk5Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++ Hands-On: Complete the Roles ++++\n",
        "\n",
        "# Define the conversation using the role structure\n",
        "messages = [\n",
        "    {\"role\": \"----\", \"content\": \"You are a math tutor who explains concepts clearly and step by step.\"},  # Complete the system role\n",
        "    {\"role\": \"----\", \"content\": \"How do you calculate the arithmetic mean of a set of numbers?\"},  # Complete the user role\n",
        "    {\"role\": \"----\", \"content\": \"To calculate the arithmetic mean:\\n\"\n",
        "                                \"1. Add all the numbers in the set.\\n\"\n",
        "                                \"2. Divide the sum by the number of numbers.\\n\\n\"\n",
        "                                \"For example, for 10, 20, and 30:\\n\"\n",
        "                                \"Mean = (10 + 20 + 30) / 3 = 60 / 3 = 20.\"\n",
        "                                }  # Optional predefined assistant response\n",
        "]\n",
        "\n",
        "# Uncomment the code below after completing the placeholders\n",
        "# response = openai.ChatCompletion.create(\n",
        "#     model=\"gpt-4\",  # Specify the model\n",
        "#     messages=messages  # Pass the completed conversation\n",
        "# )\n",
        "# print(\"Assistant's Response:\", response['choices'][0]['message']['content'])\n"
      ],
      "metadata": {
        "id": "ZZ-NuE2VhisI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Function Playground: Simplify OpenAI API Calls\n",
        "\n",
        "In OpenAI's API, you often send messages to get responses. Instead of repeating the call logic every time, you can use a function to streamline the process:\n",
        "- ‚û°Ô∏è **Input**: A list of messages containing roles and content.\n",
        "- ‚û°Ô∏è **Output**: The assistant's response as a string.\n",
        "\n",
        "üí° **Why Use a Function**:  \n",
        "Using a function keeps your code clean, reusable, and easy to maintain."
      ],
      "metadata": {
        "id": "1vUT-S7sly5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Call_ChatGPT(messages, model=\"gpt-4\", max_tokens=100, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Send messages to the OpenAI API and get the assistant's response.\n",
        "\n",
        "    Parameters:\n",
        "    - messages (list): A list of message dictionaries with 'role' and 'content'.\n",
        "    - model (str): The model to use (default is \"gpt-4\").\n",
        "    - max_tokens (int): The maximum number of tokens to include in the response.\n",
        "    - temperature (float): The sampling temperature (0.0 to 1.0, higher means more creative).\n",
        "\n",
        "    Returns:\n",
        "    - str: The assistant's response content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        # Return the assistant's response\n",
        "        return response['choices'][0]['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "4DVap39pk6_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant who provides concise answers.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the square root of 144?\"}\n",
        "]\n",
        "\n",
        "# Call the function with optional parameters\n",
        "response = Call_ChatGPT(messages, model=\"gpt-4\", max_tokens=50, temperature=0.5)\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response)\n"
      ],
      "metadata": {
        "id": "s_wlprJElx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e1k-PdhUnCbf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Hands-On: Using `Call_ChatGPT`\n",
        "\n",
        "### Task 1: Call the Function Without Adjusting Options\n",
        "1. Define the **system role** (e.g., \"You are a history expert\").\n",
        "2. Define the **user question** (e.g., \"Who invented the telephone?\").\n",
        "3. Use the `Call_ChatGPT` function without modifying its options.\n",
        "4. Run the cell and observe the response."
      ],
      "metadata": {
        "id": "Z-2soDrDnCr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++ Hands-On: Call ChatGPT Without Adjusting Options ++++\n",
        "\n",
        "# Define the messages\n",
        "messages = [\n",
        "    {\"role\": \"----\", \"content\": \"[Fill in the system role, e.g., 'You are a history expert who provides brief responses.']\"},\n",
        "    {\"role\": \"----\", \"content\": \"[Fill in the user question, e.g., 'Who was the first President of the United States?']\"}\n",
        "]\n",
        "\n",
        "# Call the function without adjusting options\n",
        "response = Call_ChatGPT(messages)\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response)\n"
      ],
      "metadata": {
        "id": "Jo5cr_pxnEav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßÆ Hands-On: Using `Call_ChatGPT`\n",
        "### Task 2: Call the Function With Custom Options\n",
        "1. Define the **system role** and craft your own **user question**.\n",
        "2. Adjust the optional parameters:\n",
        "   - `model` (e.g., `\"gpt-4\"`),\n",
        "   - `max_tokens` (e.g., `50`),\n",
        "   - `temperature` (e.g., `0.5`).\n",
        "3. Run the function and see how the custom options affect the response.\n"
      ],
      "metadata": {
        "id": "KT8LGSlHnfvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ++++ Hands-On: Call ChatGPT With Custom Options ++++\n",
        "\n",
        "# Define the messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math expert who provides step-by-step solutions to complex problems.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Solve the equation 3x^2 - 12x + 9 = 0 and explain each step.\"}\n",
        "]\n",
        "\n",
        "\n",
        "# Call the function with custom options\n",
        "response = Call_ChatGPT(\n",
        "    messages,\n",
        "    model=\"----\",         # Specify the model, e.g., \"gpt-4\"\n",
        "    max_tokens=----,      # Specify the max tokens, e.g., 50\n",
        "    temperature=----      # Specify the temperature, e.g., 0.5\n",
        ")\n",
        "\n",
        "# Print the assistant's response\n",
        "print(\"Assistant's Response:\", response)\n"
      ],
      "metadata": {
        "id": "DSpRyzzgnhea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåÑ OpenAI SDK: Generate an Image\n",
        "\n",
        "In this section, we explore how to generate images using the **OpenAI SDK**. This functionality enables the creation of visuals from descriptive prompts, turning text-based ideas into compelling imagery.\n",
        "\n",
        "### Key Steps:\n",
        "1. **Craft a Prompt**: Write a detailed and imaginative description of the image you want to create.\n",
        "   - Example: \"A futuristic city with glowing skyscrapers, flying cars, and a vibrant sunset in the background.\"\n",
        "2. **Generate the Image**: Use the OpenAI SDK to bring your prompt to life.\n",
        "3. **Evaluate**: Analyze how well the generated image aligns with your prompt and reflect on potential refinements.\n",
        "\n",
        "<font color=\"#BFBB9B\">Let's turn ideas into visuals! üñºÔ∏è</font>\n"
      ],
      "metadata": {
        "id": "zWsq_NfEomLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "def generate_and_display_images(prompt, n=1, size=\"512x512\", response_format=\"url\"):\n",
        "    \"\"\"\n",
        "    Generate and display images using OpenAI's API.\n",
        "\n",
        "    Parameters:\n",
        "    - prompt (str): A detailed description of the desired image.\n",
        "    - n (int): Number of images to generate (default is 1).\n",
        "    - size (str): Dimensions of the image (e.g., \"256x256\", \"512x512\", or \"1024x1024\").\n",
        "    - response_format (str): The format of the response, either \"url\" (default) or \"b64_json\".\n",
        "\n",
        "    Displays:\n",
        "    - Images directly in the notebook if response_format is \"url\".\n",
        "    - Prints base64 strings if response_format is \"b64_json\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate the images\n",
        "        response = openai.Image.create(\n",
        "            prompt=prompt,\n",
        "            n=n,\n",
        "            size=size,\n",
        "            response_format=response_format\n",
        "        )\n",
        "        # Process and display the images\n",
        "        if response_format == \"url\":\n",
        "            urls = [item[\"url\"] for item in response[\"data\"]]\n",
        "            for url in urls:\n",
        "                res = requests.get(url, stream=True)\n",
        "                if res.status_code == 200:\n",
        "                    img = Image.open(res.raw)\n",
        "                    display(img)\n",
        "                else:\n",
        "                    print(\"Failed to load the image from URL:\", url)\n",
        "        elif response_format == \"b64_json\":\n",
        "            for i, item in enumerate(response[\"data\"]):\n",
        "                print(f\"Base64 Image {i + 1}: {item['b64_json'][:100]}...\")  # Print first 100 chars\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "9fOc_Xmto3vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 1\n",
        "generate_and_display_images(\"A serene forest with a waterfall and sunlight streaming through the trees.\")"
      ],
      "metadata": {
        "id": "7VK6vvCEpBOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example 2\n",
        "generate_and_display_images(\"Show a venn diagram to explain prime and odd numbers.\", n=1, size=\"512x512\")"
      ],
      "metadata": {
        "id": "bD0UjRt1qiid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Hands-On: Generate and Display Scientific Images\n",
        "\n",
        "1. Define your scientific **prompt** (e.g., \"The anatomy of the human eye showing the cornea, lens, and retina\").\n",
        "2. Set the **number of images** (`n`) and **size** (e.g., `\"512x512\"`).\n",
        "3. Call the function to generate and display the image.\n",
        "\n",
        "Fill in the placeholders and run the code below.\n"
      ],
      "metadata": {
        "id": "PlVCWeJyrfEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the scientific prompt, number of images, and image size\n",
        "prompt = \"----\"  # Example: \"The structure of a DNA molecule showing the double helix\"\n",
        "n = ----  # Example: 1 or 2\n",
        "size = \"----\"  # Example: \"512x512\"\n",
        "response_format = \"----\"  # \"url\" or \"b64_json\"\n",
        "\n",
        "# Call the function with defined parameters\n",
        "generate_and_display_images(prompt, n=n, size=size, response_format=response_format)\n"
      ],
      "metadata": {
        "id": "W4sT8MbVrfan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ OpenAI SDK: Create Vector Embeddings\n",
        "\n",
        "In OpenAI's API, you can create **vector embeddings**, which convert text into numerical representations that machines can process. These embeddings are useful for various applications like search, recommendation systems, and clustering.\n",
        "\n",
        "- ‚û°Ô∏è **Text**: The input text you want to convert into a vector.\n",
        "- ‚û°Ô∏è **Model**: The model used for creating embeddings (e.g., \"text-embedding-ada-002\").\n",
        "- ‚û°Ô∏è **Embeddings**: Numerical vectors representing the input text.\n",
        "\n",
        "üí° **Why Vector Embeddings Matter**:  \n",
        "Vector embeddings allow machines to understand text in a numerical format, enabling tasks like text similarity, semantic search, and clustering.\n",
        "\n",
        "Let‚Äôs explore how to generate vector embeddings in the next examples!\n",
        "\n",
        "### Example 1: Create Vector Embedding for Text\n"
      ],
      "metadata": {
        "id": "YTwMAsKcsDZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Create a vector embedding for a single scientific sentence\n",
        "\n",
        "# Define the text you want to convert into a vector embedding\n",
        "text = \"The water cycle involves processes like evaporation, condensation, and precipitation.\"\n",
        "\n",
        "# Create the embedding using OpenAI's API\n",
        "embedding = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Print the generated embedding (vector) for the text\n",
        "# The embedding is a list of numbers that represents the text in vector form\n",
        "print(embedding['data'][0]['embedding'])\n"
      ],
      "metadata": {
        "id": "zDyN9uNasD5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Create vector embeddings for multiple scientific sentences\n",
        "\n",
        "# Define a list of scientific sentences\n",
        "texts = [\n",
        "    \"Photosynthesis is the process by which plants convert light energy into chemical energy.\",\n",
        "    \"The theory of evolution by natural selection was proposed by Charles Darwin.\"\n",
        "]\n",
        "\n",
        "# Create the embeddings for all sentences using OpenAI's API\n",
        "embeddings = openai.Embedding.create(input=texts, model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Print the embeddings for each sentence\n",
        "# Each sentence will have its own vector representation\n",
        "for i, emb in enumerate(embeddings['data']):\n",
        "    print(f\"Embedding for Sentence {i + 1}: {emb['embedding'][:10]}...\")  # Print first 10 elements of the vector for brevity\n"
      ],
      "metadata": {
        "id": "j5_6PGDPsJCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 3: Demonstrating vector embeddings with text similarity\n",
        "\n",
        "# Define two scientific sentences\n",
        "text1 = \"The Earth orbits around the Sun in an elliptical orbit.\"\n",
        "text2 = \"The Sun is the central star around which Earth revolves in an elliptical path.\"\n",
        "\n",
        "# Generate vector embeddings for both sentences\n",
        "embedding1 = openai.Embedding.create(input=text1, model=\"text-embedding-ada-002\")\n",
        "embedding2 = openai.Embedding.create(input=text2, model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Extract the embeddings (vectors) for comparison\n",
        "vector1 = embedding1['data'][0]['embedding']\n",
        "vector2 = embedding2['data'][0]['embedding']\n",
        "\n",
        "# Calculate the cosine similarity between the two vectors\n",
        "import numpy as np\n",
        "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "# Print the cosine similarity (ranges from -1 to 1, where 1 means very similar)\n",
        "print(f\"Cosine Similarity between the two sentences: {cosine_similarity}\")\n"
      ],
      "metadata": {
        "id": "4f12ASe7s20l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Hands-On: Working with Vector Embeddings\n",
        "\n",
        "### Task 1: Generate a Vector Embedding for a Sentence\n",
        "1. Define a scientific sentence (e.g., \"The law of gravity states that every object attracts every other object\").\n",
        "2. Generate the vector embedding for this sentence.\n",
        "3. Print the embedding to observe the numerical representation.\n",
        "\n",
        "### Task 2: Measure Similarity Between Two Sentences\n",
        "1. Define two scientific sentences (e.g., about physics).\n",
        "2. Generate vector embeddings for both sentences.\n",
        "3. Calculate and print the cosine similarity between the two embeddings.\n",
        "\n",
        "Fill in the code below and run it to complete these tasks.\n"
      ],
      "metadata": {
        "id": "oYNf96p9tp86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Generate vector embedding for a single scientific sentence\n",
        "\n",
        "# Define the text for vector embedding\n",
        "text = \"----\"  # Example: \"The law of gravity states that every object attracts every other object.\"\n",
        "\n",
        "# Create the embedding using OpenAI's API\n",
        "embedding = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Print the generated embedding (vector)\n",
        "print(embedding['data'][0]['embedding'])\n",
        "\n"
      ],
      "metadata": {
        "id": "teNf2vsVtAux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Task 2: Measure similarity between two scientific sentences\n",
        "\n",
        "# Define two scientific sentences\n",
        "text1 = \"----\"  # Example: \"Newton's laws of motion describe the relationship between a body and the forces acting on it.\"\n",
        "text2 = \"----\"  # Example: \"The force of an object is equal to its mass times acceleration.\"\n",
        "\n",
        "# Generate vector embeddings for both sentences\n",
        "embedding1 = openai.Embedding.create(input=text1, model=\"text-embedding-ada-002\")\n",
        "embedding2 = openai.Embedding.create(input=text2, model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Extract the embeddings (vectors) for comparison\n",
        "vector1 = embedding1['data'][0]['embedding']\n",
        "vector2 = embedding2['data'][0]['embedding']\n",
        "\n",
        "# Calculate the cosine similarity between the two vectors\n",
        "import numpy as np\n",
        "cosine_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "\n",
        "# Print the cosine similarity (ranges from -1 to 1, where 1 means very similar)\n",
        "print(f\"Cosine Similarity between the two sentences: {cosine_similarity}\")"
      ],
      "metadata": {
        "id": "zTddU0FvtiHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}